# 条件期望（Conditional Expectation）

## 1. 一句话
- 条件期望 $E[Y|X]$ 是"已知 $X$ 的信息后，对 $Y$ 的最优预测"，本质是关于 $X$ 的函数（随机变量）

---

## 2. 两个层次的定义

### 层次1：给定 $X=x$ 的数值期望
$$E[Y | X=x] = \begin{cases}
\sum_y y \cdot P(Y=y | X=x) & \text{离散} \\
\int y \cdot p(y|x) \, dy & \text{连续}
\end{cases}$$

**这是一个数**：固定 $x$ 后，对 $y$ 的条件分布求期望。

**例子**：$(X,Y)$ 服从二元正态分布，$X \sim \mathcal{N}(0,1)$，$Y|X=x \sim \mathcal{N}(\rho x, 1-\rho^2)$

$$E[Y|X=x] = \rho x$$

---

### 层次2：条件期望随机变量
$$E[Y|X] = g(X)$$

其中 $g(x) = E[Y|X=x]$。

**关键理解**：
- $E[Y|X]$ **是随机变量**（因为是 $X$ 的函数）
- $E[Y|X=x]$ 是数值
- $E[Y|X]$ 把每个 $X$ 的取值映射到对应的条件期望

**上例延续**：
$$E[Y|X] = \rho X \quad \text{（这是随机变量）}$$

---

## 3. 条件期望的性质

### 全期望公式（Tower Property）
$$E[Y] = E[E[Y|X]]$$

**含义**：
- 内层 $E[Y|X]$ 是关于 $X$ 的随机变量
- 外层再对 $X$ 求期望
- 结果等于直接对 $Y$ 求期望

**证明思路**（连续情形）：
$$E[E[Y|X]] = \int E[Y|X=x] \cdot p(x) \, dx$$
$$= \int \left( \int y \cdot p(y|x) dy \right) p(x) \, dx$$
$$= \iint y \cdot p(y|x) p(x) \, dy \, dx$$
$$= \iint y \cdot p(x,y) \, dy \, dx \quad \text{（链式法则）}$$
$$= \int y \cdot p(y) \, dy = E[Y]$$

---

### 条件期望的线性性
$$E[aY_1 + bY_2 | X] = a E[Y_1|X] + b E[Y_2|X]$$

---

### 已知变量"提出来"
若 $g(X)$ 是 $X$ 的函数，则：
$$E[g(X) Y | X] = g(X) \cdot E[Y|X]$$

**直觉**：已知 $X$ 后，$g(X)$ 是常数，可以提到期望外面。

---

### 独立性简化
若 $X, Y$ 独立，则：
$$E[Y|X] = E[Y]$$

**含义**：$X$ 的信息对预测 $Y$ 没有帮助。

---

## 4. 条件期望作为最优预测

**问题**：用 $X$ 的函数 $h(X)$ 预测 $Y$，最小化均方误差：
$$\min_{h} E[(Y - h(X))^2]$$

**答案**：
$$h^*(X) = E[Y|X]$$

---

### 证明

**步骤1：用全期望公式展开**
$$E[(Y - h(X))^2] = E[E[(Y - h(X))^2 | X]]$$

**步骤2：将积分写明**
$$= \int E[(Y - h(x))^2 | X=x] \cdot p(x) \, dx$$

**直觉**：总体 MSE = 每个 $x$ 的局部 MSE 的加权平均。

**步骤3：逐点最小化**

由于 $p(x) \geq 0$ 是权重，最小化总体等价于最小化每个局部：
$$\min_{h(x)} E[(Y - h(x))^2 | X=x]$$

这是关于固定 $x$ 下 $Y$ 的条件分布的优化问题。

**步骤4：求解单点 MSE**

固定 $X=x$，令 $c = h(x)$，求：
$$\min_c E[(Y - c)^2 | X=x]$$

展开：
$$E[(Y - c)^2 | X=x] = E[Y^2 | X=x] - 2c \cdot E[Y | X=x] + c^2$$

对 $c$ 求导：
$$\frac{\partial}{\partial c} = -2E[Y | X=x] + 2c = 0$$

得：
$$c^* = E[Y | X=x]$$

**步骤5：结论**

对每个 $x$，最优值是 $h^*(x) = E[Y|X=x]$，因此：
$$h^*(X) = E[Y|X]$$

---

### 关键理解

**全局最优 = 逐点最优的组合**

$$E[(Y - h(X))^2] = \int \underbrace{E[(Y - h(x))^2 | X=x]}_{\text{局部损失}} \cdot p(x) \, dx$$

由于积分是加权和，最小化总体损失等价于最小化每个局部损失。

---

**应用**：
- VAE decoder：$f^*(z) = E[X|Z=z]$
- 回归问题：$f^*(x) = E[Y|X=x]$

---

## 5. 嵌套条件期望

### 条件化的条件期望
$$E[E[Y|X,Z] | X] = E[Y|X]$$

**直觉**：
- 先知道 $(X,Z)$，得到 $E[Y|X,Z]$
- 再"忘记" $Z$，只保留 $X$ 的信息
- 结果等于直接条件化在 $X$ 上

---

### 带条件的全期望公式
$$E[Y|X] = E[E[Y|X,Z] | X]$$

**应用**：VAE 推导中的关键步骤
$$E_{z \sim p(z|x)}[f(x,z)] = E[f(x,Z)|X=x]$$

---

## 6. 计算方法

### 方法1：直接定义
$$E[Y|X=x] = \int y \cdot p(y|x) \, dy$$

**需要知道**：条件分布 $p(y|x)$

---

### 方法2：联合分布
$$p(y|x) = \frac{p(x,y)}{p(x)}$$

$$E[Y|X=x] = \frac{\int y \cdot p(x,y) \, dy}{p(x)}$$

---

### 方法3：回归函数
若 $(X,Y)$ 联合分布已知，$E[Y|X]$ 是使 MSE 最小的 $X$ 的函数。

**实际中**：通过训练神经网络 $f_\theta(x)$ 逼近 $E[Y|X]$。

---

## 7. 在机器学习中的应用

### VAE 的最优 decoder
训练时最小化：
$$E_{x \sim p_{data}} E_{z \sim q_\phi(z|x)} [\|x - f_\theta(z)\|^2]$$

固定 $q_\phi$，最优 $f^*_\theta(z)$：
$$f^*(z) = E_{q_\phi(x|z)}[x]$$

这里 $q_\phi(x|z)$ 由 Bayes 公式得到：
$$q_\phi(x|z) = \frac{q_\phi(z|x) p_{data}(x)}{q_\phi(z)}$$

---

### 强化学习的值函数
$$V(s) = E[R | S=s]$$
$$Q(s,a) = E[R | S=s, A=a]$$

都是条件期望。

---

### 生成模型的条件生成
$$p_\theta(x|c) \quad \text{（CVAE, conditional GAN）}$$

目标是学习 $E[X|C=c]$ 或完整的 $p(x|c)$。

---

## 8. 与无条件期望的对比

| 概念 | 无条件 | 条件 |
|------|--------|------|
| 定义 | $E[Y] = \int y p(y) dy$ | $E[Y\|X=x] = \int y p(y\|x) dy$ |
| 类型 | 数值 | $E[Y\|X=x]$ 是数，$E[Y\|X]$ 是随机变量 |
| 全期望 | - | $E[Y] = E[E[Y\|X]]$ |
| 最优预测 | 常数预测：$E[Y]$ | 用 $X$ 预测：$E[Y\|X]$ |

---

## 9. 相关模块

- [期望](Expectation.md)：无条件期望的定义
- [嵌套期望](IteratedExpectation.md)：全期望公式详解
- [链式法则](ChainRule.md)：$p(x,y) = p(x)p(y|x)$
- [Bayes公式](BayesTheorem.md)：$p(y|x) = \frac{p(x|y)p(y)}{p(x)}$

---

## 10. 速查

| 公式 | 含义 |
|------|------|
| $E[Y\|X=x] = \int y p(y\|x) dy$ | 给定 $x$ 的条件期望（数值） |
| $E[Y\|X]$ | 条件期望（随机变量） |
| $E[Y] = E[E[Y\|X]]$ | 全期望公式 |
| $E[g(X)Y\|X] = g(X)E[Y\|X]$ | 已知变量提出 |
| $\min_h E[(Y-h(X))^2]$ 最优解 | $h^*(X) = E[Y\|X]$ |

---

## 11. 常见误区

- **误区1**：认为 $E[Y|X]$ 是数（忘记它是随机变量）
- **误区2**：混淆 $E[Y|X]$ 和 $E[Y|X=x]$
- **误区3**：认为 $E[XY|X] = X \cdot E[Y|X]$ 需要独立性（不需要，$X$ 已知）
- **误区4**：全期望公式用反了：$E[E[Y|X]] = E[Y]$ ✓，$E[E[X|Y]] \neq E[X]$ ✗（等式依然成立，但理解反了）

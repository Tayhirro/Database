# “线性 / 非线性”到底在说什么（以及对哪个变量线性）

## 1. 数学上的“线性”（线性映射）
设 `V,W` 是向量空间，`f: V -> W`。

`f` 称为**线性映射（linear map）**，如果对任意 `x1,x2∈V` 与标量 `a,b`：
- `f(a x1 + b x2) = a f(x1) + b f(x2)`

等价分解成两条更好用的判断：
- **可加性**：`f(x1+x2)=f(x1)+f(x2)`
- **齐次性**：`f(a x)=a f(x)`

典型例子（有限维）：`f(x)=Ax`。

### 1.1 仿射（affine）≠ 线性
- `f(x)=Ax+b` 称为仿射（affine）：满足“直线映到直线”，但一般不满足 `f(0)=0`，因此不是线性映射。

### 1.2 多输入时：一定要说“对哪个变量线性”
如果 `g` 有多个输入（比如 `g(u,v)` 或 `g(q,k,v)`），常见情况是：
- **对某个变量线性**：固定其它变量，把它看成单变量函数时是线性的
- **整体线性**：把所有输入拼在一个大向量里，作为一个函数整体仍满足线性

例如双线性（bilinear）：`g(u,v)` 对 `u` 固定 `v` 时线性、对 `v` 固定 `u` 时也线性，但对 `(u,v)` 整体一般不是线性映射。

---

## 2. 机器学习里“线性/非线性”的两种口径（别混）
在 ML 语境里，“线性模型”常见有两种说法：

1) **对输入 `x` 线性**：例如 `f(x)=w^T x + b`（决策边界是超平面）
2) **对参数线性**：例如 `f(x;w)=w^T φ(x) + b` 对 `w` 是线性的，但对 `x` 可能很非线性（取决于 `φ`）

你看到一句“把非线性问题变线性”，通常指的是：  
在原始输入空间里需要非线性的决策边界，但通过特征映射 `φ` 后，在特征空间里可以用线性模型解决（并且计算只用到内积/Gram 矩阵）。

---

## 3. 核方法里“线性/非线性”具体指什么
把输入 `x` 送到特征空间：`φ: X -> H`（`H` 是某个内积空间），然后做线性模型：
- `f(x) = <w, φ(x)> + b`

这对 `w`（以及对 `φ(x)`）是线性的；但如果 `φ` 是非线性的，那么 `f` 作为 `x` 的函数通常就是非线性的。

用核技巧把内积写成核函数 `k(x,y)=<φ(x),φ(y)>` 后，很多方法的预测写成：
- `f(x) = Σ_i α_i k(x_i, x) + b`

这里“线性”通常指：对系数 `α` 是线性的；而 `x -> k(x_i,x)` 往往是非线性的，所以在输入空间的几何效果会是非线性的。

相关：`math/线性代数/modules/KernelMethods.md`

---

## 4. 注意力（attention）里“对谁线性 / 整体非线性”的最小说明
把注意力（单头简化）写成：
- `Y = A V`
- `A = softmax(Q K^T / sqrt(d))`

### 4.1 固定权重 `A` 时：对 `V` 线性
若把 `A` 当作常数矩阵，则 `V -> A V` 是线性映射（就是矩阵乘法）。

### 4.2 但整体（从输入到输出）不是线性
真实模型里 `A` 不是常数，而是由 `Q,K`（进一步由输入 `X`）算出来的；并且含有：
- `Q K^T`（输入之间相乘，出现双线性/二次型结构）
- `softmax(·)`（非线性）

所以从输入 `X` 到输出 `Y` 的整体映射通常不是线性映射。

### 4.3 一个 2-key 的数值反例（看齐次性直接失败）
取标量情形（`d=1`）：令
- `q=1, k1=0, k2=1, v1=1, v2=0`

则权重 `a1 = exp(q k1)/(exp(q k1)+exp(q k2)) = 1/(1+e)`，输出
- `y = a1 v1 + (1-a1) v2 = 1/(1+e)`

把所有量都乘 2（对应“缩放输入”会同时缩放 `Q,K,V` 的一种极简情形）：
- `q'=2, k1'=0, k2'=2, v1'=2, v2'=0`

则 `y' = 2/(1+e^4)`，而 `2y = 2/(1+e)`；两者不相等，所以不满足齐次性，整体不是线性。

